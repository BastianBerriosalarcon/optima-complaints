{
  "name": "Knowledge - Storage Finalizer",
  "description": "Workflow enfocado en almacenar chunks con embeddings en la BD y finalizar procesamiento. Principio SRP: Una sola responsabilidad - storage.",
  "tags": ["rag", "knowledge", "storage", "srp"],
  "nodes": [
    {
      "parameters": {
        "workflowId": "{{ $json.workflowId }}"
      },
      "id": "workflow-trigger",
      "name": "Trigger del Workflow",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1,
      "position": [240, 300]
    },
    {
      "parameters": {
        "functionCode": "// Función enfocada en preparar chunks para almacenamiento\n// Principio SRP: Una sola responsabilidad - preparar datos para BD\n\nconst storageData = $input.first().json;\n\n// Validar datos de entrada\nif (!storageData.chunks_with_embeddings || !Array.isArray(storageData.chunks_with_embeddings)) {\n  throw new Error('Datos de chunks con embeddings inválidos o faltantes');\n}\n\nif (storageData.chunks_with_embeddings.length === 0) {\n  throw new Error('No hay chunks con embeddings para almacenar');\n}\n\n// Preparar chunks para inserción en BD\nconst chunksForDB = storageData.chunks_with_embeddings.map(chunk => {\n  // Validar embedding\n  if (!chunk.embedding || !Array.isArray(chunk.embedding) || chunk.embedding.length !== 768) {\n    throw new Error(`Embedding inválido para chunk ${chunk.chunk_index}`);\n  }\n  \n  return {\n    documento_id: chunk.documento_id,\n    chunk_index: chunk.chunk_index,\n    contenido: chunk.contenido,\n    embedding: JSON.stringify(chunk.embedding), // Supabase requiere JSON string para vector\n    token_count: chunk.token_count,\n    metadata: chunk.metadata\n  };\n});\n\n// Preparar estadísticas para actualizar documento\nconst finalStats = {\n  total_chunks: chunksForDB.length,\n  total_tokens: chunksForDB.reduce((sum, chunk) => sum + chunk.token_count, 0),\n  avg_tokens_per_chunk: Math.round(\n    chunksForDB.reduce((sum, chunk) => sum + chunk.token_count, 0) / chunksForDB.length\n  ),\n  embedding_model: storageData.embedding_stats?.embedding_model || 'gemini-embedding-001',\n  processed_at: new Date().toISOString()\n};\n\nconsole.log('Chunks preparados para almacenamiento:', {\n  documentId: storageData.document_id,\n  tenantId: storageData.tenant_id,\n  ...finalStats\n});\n\nreturn [{\n  json: {\n    // Datos del documento\n    document_id: storageData.document_id,\n    tenant_id: storageData.tenant_id,\n    document_title: storageData.document_title,\n    \n    // Chunks listos para BD\n    chunks_for_db: chunksForDB,\n    \n    // Estadísticas finales\n    final_stats: finalStats\n  }\n}];"
      },
      "id": "prepare-chunks-for-storage",
      "name": "Preparar para Almacenamiento",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [460, 300]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO documento_chunks (documento_id, chunk_index, contenido, embedding, token_count, metadata) SELECT * FROM UNNEST($1::uuid[], $2::integer[], $3::text[], $4::vector[], $5::integer[], $6::jsonb[])",
        "additionalFields": {
          "queryParameters": "={{ [\n  $json.chunks_for_db.map(c => c.documento_id),\n  $json.chunks_for_db.map(c => c.chunk_index),\n  $json.chunks_for_db.map(c => c.contenido),\n  $json.chunks_for_db.map(c => c.embedding),\n  $json.chunks_for_db.map(c => c.token_count),\n  $json.chunks_for_db.map(c => JSON.stringify(c.metadata))\n] }}"
        }
      },
      "id": "insert-chunks-batch",
      "name": "Insertar Chunks (Lote)",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2,
      "position": [680, 300]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE documentos_conocimiento SET estado = $1, fecha_procesado = $2, chunks_generados = $3, metadatos = jsonb_set(metadatos, '{processing_stats}', $4::jsonb) WHERE id = $5",
        "additionalFields": {
          "queryParameters": "={{ ['activo', new Date().toISOString(), $('Preparar para Almacenamiento').first().json.final_stats.total_chunks, JSON.stringify($('Preparar para Almacenamiento').first().json.final_stats), $('Preparar para Almacenamiento').first().json.document_id] }}"
        }
      },
      "id": "update-document-status",
      "name": "Actualizar Estado Documento",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2,
      "position": [900, 300]
    },
    {
      "parameters": {
        "functionCode": "// Función enfocada en generar reporte final\n// Principio SRP: Una sola responsabilidad - generar resumen\n\nconst preparationData = $('Preparar para Almacenamiento').first().json;\nconst insertResult = $('Insertar Chunks (Lote)').first().json;\nconst updateResult = $input.first().json;\n\n// Verificar que la inserción fue exitosa\nconst chunksInserted = insertResult.length || 0;\nconst expectedChunks = preparationData.chunks_for_db.length;\n\nif (chunksInserted === 0) {\n  throw new Error('No se insertaron chunks en la base de datos');\n}\n\n// Generar reporte final del procesamiento\nconst finalReport = {\n  status: 'completed',\n  document_processing: {\n    document_id: preparationData.document_id,\n    tenant_id: preparationData.tenant_id,\n    document_title: preparationData.document_title,\n    final_status: 'activo'\n  },\n  \n  processing_summary: {\n    chunks_created: expectedChunks,\n    chunks_stored: chunksInserted,\n    total_tokens_processed: preparationData.final_stats.total_tokens,\n    avg_tokens_per_chunk: preparationData.final_stats.avg_tokens_per_chunk,\n    embedding_model: preparationData.final_stats.embedding_model\n  },\n  \n  pipeline_performance: {\n    ingestion_completed: true,\n    chunking_completed: true,\n    embedding_completed: true,\n    storage_completed: true,\n    pipeline_success: chunksInserted === expectedChunks\n  },\n  \n  timestamps: {\n    processing_completed_at: new Date().toISOString(),\n    ready_for_rag_queries: true\n  }\n};\n\n// Log final del procesamiento\nconsole.log('Procesamiento RAG completado exitosamente:', {\n  documentId: finalReport.document_processing.document_id,\n  tenantId: finalReport.document_processing.tenant_id,\n  chunksStored: finalReport.processing_summary.chunks_stored,\n  pipelineSuccess: finalReport.pipeline_performance.pipeline_success\n});\n\nreturn [{ json: finalReport }];"
      },
      "id": "generate-final-report",
      "name": "Generar Reporte Final",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [1120, 300]
    }
  ],
  "connections": {
    "Trigger del Workflow": {
      "main": [
        [
          {
            "node": "Preparar para Almacenamiento",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Preparar para Almacenamiento": {
      "main": [
        [
          {
            "node": "Insertar Chunks (Lote)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insertar Chunks (Lote)": {
      "main": [
        [
          {
            "node": "Actualizar Estado Documento",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Actualizar Estado Documento": {
      "main": [
        [
          {
            "node": "Generar Reporte Final",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "saveExecutionProgress": true,
    "saveManualExecutions": true,
    "callerPolicy": "workflowsFromSameOwner"
  }
}