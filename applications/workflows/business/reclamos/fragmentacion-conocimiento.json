{
  "name": "Knowledge - Document Chunking",
  "description": "Workflow enfocado en chunkear documentos en fragmentos para embeddings. Principio SRP: Una sola responsabilidad - chunking.",
  "tags": ["rag", "knowledge", "chunking", "srp"],
  "nodes": [
    {
      "parameters": {
        "workflowId": "{{ $json.workflowId }}"
      },
      "id": "workflow-trigger",
      "name": "Trigger del Workflow",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1,
      "position": [240, 300]
    },
    {
      "parameters": {
        "functionCode": "// Función enfocada en chunkear documentos\n// Principio SRP: Una sola responsabilidad - dividir texto en chunks\n\nconst documentData = $input.first().json;\n\n// Validar datos de entrada\nif (!documentData.document_id || !documentData.content_to_chunk) {\n  throw new Error('Datos requeridos faltantes: document_id o content_to_chunk');\n}\n\n// Configuración de chunking siguiendo mejores prácticas RAG\nconst CHUNK_SIZE = 512; // tokens aproximados (óptimo para embeddings)\nconst OVERLAP = 50; // overlap entre chunks para contexto\n\n// Función para dividir texto en chunks con overlap\nfunction chunkTextWithOverlap(text, chunkSize, overlap) {\n  // Dividir por palabras para mejor control\n  const words = text.split(/\\s+/).filter(word => word.length > 0);\n  const chunks = [];\n  let startIndex = 0;\n  \n  while (startIndex < words.length) {\n    const endIndex = Math.min(startIndex + chunkSize, words.length);\n    const chunkWords = words.slice(startIndex, endIndex);\n    const chunkText = chunkWords.join(' ');\n    \n    if (chunkText.trim().length > 0) {\n      chunks.push({\n        contenido: chunkText.trim(),\n        start_word_index: startIndex,\n        end_word_index: endIndex,\n        word_count: chunkWords.length,\n        estimated_tokens: Math.ceil(chunkWords.length * 1.3) // estimación conservadora\n      });\n    }\n    \n    // Si llegamos al final, salir\n    if (endIndex >= words.length) break;\n    \n    // Mover el índice considerando overlap\n    startIndex = endIndex - overlap;\n  }\n  \n  return chunks;\n}\n\n// Procesar chunks\nconst rawChunks = chunkTextWithOverlap(\n  documentData.content_to_chunk, \n  CHUNK_SIZE, \n  OVERLAP\n);\n\n// Preparar chunks con metadatos completos\nconst processedChunks = rawChunks.map((chunk, index) => ({\n  documento_id: documentData.document_id,\n  chunk_index: index,\n  contenido: chunk.contenido,\n  token_count: chunk.estimated_tokens,\n  metadata: {\n    // Posición en el documento\n    start_word_index: chunk.start_word_index,\n    end_word_index: chunk.end_word_index,\n    word_count: chunk.word_count,\n    \n    // Metadatos del documento original\n    document_title: documentData.document_title,\n    category: documentData.processing_config?.categoria || 'general',\n    tags: documentData.processing_config?.tags || [],\n    author: documentData.processing_config?.author || 'Sistema',\n    version: documentData.processing_config?.version || '1.0',\n    language: documentData.processing_config?.language || 'es',\n    \n    // Timestamps\n    chunked_at: new Date().toISOString(),\n    original_created_at: documentData.created_at\n  }\n}));\n\n// Estadísticas de procesamiento\nconst chunkingStats = {\n  total_chunks: processedChunks.length,\n  avg_tokens_per_chunk: Math.round(\n    processedChunks.reduce((sum, c) => sum + c.token_count, 0) / processedChunks.length\n  ),\n  total_estimated_tokens: processedChunks.reduce((sum, c) => sum + c.token_count, 0),\n  original_content_length: documentData.content_to_chunk.length,\n  overlap_used: OVERLAP,\n  chunk_size_config: CHUNK_SIZE\n};\n\nconsole.log('Documento chunkeado exitosamente:', {\n  documentId: documentData.document_id,\n  tenantId: documentData.tenant_id,\n  ...chunkingStats\n});\n\nreturn [{\n  json: {\n    // Datos del documento\n    document_id: documentData.document_id,\n    tenant_id: documentData.tenant_id,\n    document_title: documentData.document_title,\n    \n    // Chunks procesados\n    chunks: processedChunks,\n    \n    // Estadísticas\n    chunking_stats: chunkingStats,\n    \n    // Control de flujo\n    processing_step: 'embedding',\n    next_workflow: 'knowledge-embedding'\n  }\n}];"
      },
      "id": "chunk-document-content",
      "name": "Chunkear Contenido",
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [460, 300]
    },
    {
      "parameters": {
        "operation": "create",
        "resource": "execution",
        "workflowId": "knowledge-embedding",
        "data": "={{ JSON.stringify($json) }}"
      },
      "id": "trigger-embedding-workflow",
      "name": "Iniciar Embeddings",
      "type": "n8n-nodes-base.n8n",
      "typeVersion": 1,
      "position": [680, 300]
    }
  ],
  "connections": {
    "Trigger del Workflow": {
      "main": [
        [
          {
            "node": "Chunkear Contenido",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Chunkear Contenido": {
      "main": [
        [
          {
            "node": "Iniciar Embeddings",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "saveExecutionProgress": true,
    "saveManualExecutions": true,
    "callerPolicy": "workflowsFromSameOwner"
  }
}